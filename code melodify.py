# -*- coding: utf-8 -*-
"""Code Melodify (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169B0qVDT2Bv4ixk0ffvhid1aeqS5Gx3M

#Melodify: Music Similarity through Embedding and Metadata

## Team Members - Gautham Satyanarayana, Yukta Salvi, Saumya Chaudhury, Pradeep Raj, Parikha Goyanka
"""

!pip install sentence_transformers # for XGBoost Model

!pip install --upgrade google-cloud-storage # the million song subset data is hosted on GCP

from google.colab import auth
auth.authenticate_user()

# Standard libraries
import os
import string
import csv
import pathlib
import ast
import random
import warnings  # Handle warnings

# Data manipulation & preprocessing
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, MultiLabelBinarizer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, f1_score, silhouette_score
from scipy.stats import zscore

# NLP & Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
from gensim.models import Word2Vec
import gensim.downloader as api
from sentence_transformers import SentenceTransformer

# Machine learning & deep learning
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from tensorflow import keras
from tensorflow.keras import layers, models, Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import shap
import networkx as nx

# Audio processing
from scipy.io.wavfile import write

# Cloud & File handling
from google.cloud import storage
import h5py

# Ignore warnings
warnings.filterwarnings("ignore")

"""#Data Acquisition"""

# The Million Song Subset data was loaded from a Google Cloud Bucket. Due to the large size of the arrays,
# converting the data into a CSV resulted in automatic pruning of the values. To prevent any data loss,
# the data was retained in its original form within a DataFrame.
project_id = 'authentic-block-442222-g0'
bucket_name = "audio_dataset1"
stop_words = set(stopwords.words('english'))
# Initialize the client
client = storage.Client(project=project_id)
# Get the bucket
bucket = client.get_bucket(bucket_name)
# Path for the combined CSV file (this will be the final output)
combined_csv_path = '/content/drive/MyDrive/combined_outputA.csv'
# Ensure the directory exists
os.makedirs(os.path.dirname(combined_csv_path), exist_ok=True)

# @title


all_dataframes = []
def convert_h5_to_npy(file_path, i):
  with h5py.File(file_path, "r") as h5_file:
   bar_confidence = h5_file["analysis/bars_confidence"][:]
   bar_start = h5_file["analysis/bars_start"][:]
   beat_confidence = h5_file["analysis/beats_confidence"][:]
   beat_start = h5_file["analysis/beats_start"][:]
   sections_confidence = h5_file["analysis/sections_confidence"][:]
   sections_start = h5_file["analysis/sections_start"][:]
   segments_confidence = h5_file["analysis/segments_confidence"][:]
   segments_loudness_max = h5_file["analysis/segments_loudness_max"][:]
   segments_loudness_max_time = h5_file["analysis/segments_loudness_max_time"][:]
   segments_loudness_start = h5_file["analysis/segments_loudness_start"][:]
   segments_start = h5_file["analysis/segments_start"][:]
   segments_timre = h5_file["analysis/segments_timbre"][:]
   songs = h5_file["analysis/songs"][:]
   tatums_confidence = h5_file["analysis/tatums_confidence"][:]
   tatums_start = h5_file["analysis/tatums_start"][:]
   artist_terms = h5_file["metadata/artist_terms"][:]
   artist_terms_list = [s.decode('utf-8') for s in artist_terms.tolist()]
   artist_term_freq = h5_file["metadata/artist_terms_freq"][:]
   artist_term_weight = h5_file["/metadata/artist_terms_weight"][:]
   similar_artists = h5_file["/metadata/similar_artists"][:]
   similar_artist_list = [s.decode('utf-8') for s in similar_artists.tolist()]
   songs_metadata = h5_file["/metadata/songs"][:]
   artist_mbtags = h5_file["/musicbrainz/artist_mbtags"][:]
   artist_mbtags_counts = h5_file["/musicbrainz/artist_mbtags_count"][:]
   songs_detials = h5_file["/musicbrainz/songs"][:]
  d1 = {'bar_confidence': [bar_confidence],
                                   'bar_start': [bar_start],
                       "beat_confidence": [beat_confidence],
                       "beat_start" : [beat_start],
                       "sections_confidence": [sections_confidence],
                       "sections_start" : [sections_start],
                       "segments_confidence" : [segments_confidence],
                       "segments_loudness_max" : [segments_loudness_max],
                       "segments_loudness_max_time" : [segments_loudness_max_time],
                       "segments_loudness_start": [segments_loudness_start],
                       "segments_start" : [segments_start],
                       "segments_timre": [segments_timre],
                       "tatums_confidence" : [tatums_confidence],
                       "tatums_start" : [tatums_start],
                       "artist_terms_list" : [artist_terms_list],
                       "artist_term_freq" : [artist_term_freq],
                       "artist_term_weight" : [artist_term_weight],
                       "similar_artists" : [similar_artist_list],
                       "artist_mbtags" : [artist_mbtags],
                        "artist_mbtags_counts" : [artist_mbtags_counts],
                        "songs_detials" : [songs_detials]
                       }
  d3 = {name: songs[name][0] for name in songs.dtype.names}
  d1.update(d3)
  d4 = {name: songs_metadata[name][0] for name in songs_metadata.dtype.names}
  d1.update(d4)
  one_hf = pd.DataFrame(d1)

  return one_hf

def combine_csv_files():
    header_written = False  # Flag to ensure header is written only once
    i = 0
    with open(combined_csv_path, mode='a', newline='') as combined_file:
        writer = csv.writer(combined_file)

        # Iterate through combinations of letters A-Z (nested loop)
        for letter1 in string.ascii_uppercase:  # A to Z
          if letter1 == 'Z':
            file_name = f'MillionSongSubset/A/Z/{letter1}/'
            destination_path = f'/content/final/A/Z/{letter1}/'
            i = 100
            if not os.path.exists(destination_path):
                    os.makedirs(destination_path)
                    # List blobs with the current prefix
            blobs = bucket.list_blobs(prefix=file_name)

                # Download each blob, convert to .npy data, and save only as CSV
            for blob in blobs:
                    # Calculate the relative path for local storage
                    relative_path = blob.name[len(file_name):]
                    local_file_path = os.path.join(destination_path, relative_path)
                    # Ensure the local directory exists before downloading
                    local_dir = os.path.dirname(local_file_path)
                    if not os.path.exists(local_dir):
                        os.makedirs(local_dir)
                    # Download the blob to the local path
                    blob.download_to_filename(local_file_path)
                    print(f"Downloaded {blob.name} to {local_file_path}")

                    csv_file_path = local_file_path.replace('.h5', '.csv')
                    df = convert_h5_to_npy(local_file_path, i)
                    all_dataframes.append(df)

                    # Delete the .h5 file after processing
                    os.remove(local_file_path)
                    print(f"Deleted H5 file: {local_file_path}")
          else:
            for letter2 in string.ascii_uppercase:  # A to Z
                # Construct the prefix dynamically
                file_name = f'MillionSongSubset/A/{letter1}/{letter2}/'
                # Set the destination path dynamically based on the current letters
                destination_path = f'/content/final/A/{letter1}/{letter2}/'

                # Ensure the local directory exists before downloading files
                if not os.path.exists(destination_path):
                    os.makedirs(destination_path)
                    # List blobs with the current prefix
                blobs = bucket.list_blobs(prefix=file_name)

                # Download each blob, convert to .npy data, and save only as CSV
                for blob in blobs:
                    # Calculate the relative path for local storage
                    relative_path = blob.name[len(file_name):]
                    local_file_path = os.path.join(destination_path, relative_path)

                    # Ensure the local directory exists before downloading
                    local_dir = os.path.dirname(local_file_path)
                    if not os.path.exists(local_dir):
                        os.makedirs(local_dir)

                    # Download the blob to the local path
                    blob.download_to_filename(local_file_path)
                    print(f"Downloaded {blob.name} to {local_file_path}")
                    csv_file_path = local_file_path.replace('.h5', '.csv')
                    df = convert_h5_to_npy(local_file_path, i)

                    # Append this DataFrame to the list for later concatenation
                    all_dataframes.append(df)



# Call the function to combine the CSV files into a single one
combine_csv_files()

combined_csv_path = '/content/drive/MyDrive/combined_outputB.csv'

def combine_csv_filesB():
   for letter1 in string.ascii_uppercase[0:9]:  # A to I
    if letter1 == 'B':
      for letter2 in string.ascii_uppercase[0:10]:  # A to J
        file_name = f'MillionSongSubset/B/B/B/{letter2}/'  # A to J
        destination_path = f'/content/final/B/B/B/{letter2}/'

        # Ensure the local directory exists before downloading files
        i = 100
        if not os.path.exists(destination_path):
                    os.makedirs(destination_path)
                    # List blobs with the current prefix
        blobs = bucket.list_blobs(prefix=file_name)

                # Download each blob, convert to .npy data, and save only as CSV
        for blob in blobs:
                    # Calculate the relative path for local storage
                    relative_path = blob.name[len(file_name):]
                    local_file_path = os.path.join(destination_path, relative_path)

                    # Ensure the local directory exists before downloading
                    local_dir = os.path.dirname(local_file_path)
                    if not os.path.exists(local_dir):
                        os.makedirs(local_dir)

                    # Download the blob to the local path
                    blob.download_to_filename(local_file_path)
                    print(f"Downloaded {blob.name} to {local_file_path}")
                    csv_file_path = local_file_path.replace('.h5', '.csv')
                    df = convert_h5_to_npy(local_file_path, 100)

                    all_dataframes.append(df)

                    # Delete the .h5 file after processing
                    os.remove(local_file_path)
                    print(f"Deleted H5 file: {local_file_path}")

    elif letter1 == 'I':
        i = 100
        file_name = f'MillionSongSubset/B/B/I/{letter2}/'  # A to J
        destination_path = f'/content/final/B/B/I/{letter2}/'
        if not os.path.exists(destination_path):
                    os.makedirs(destination_path)
                    # List blobs with the current prefix
        blobs = bucket.list_blobs(prefix=file_name)

                # Download each blob, convert to .npy data, and save only as CSV
        for blob in blobs:
                    # Calculate the relative path for local storage
                    relative_path = blob.name[len(file_name):]
                    local_file_path = os.path.join(destination_path, relative_path)

                    # Ensure the local directory exists before downloading
                    local_dir = os.path.dirname(local_file_path)
                    if not os.path.exists(local_dir):
                        os.makedirs(local_dir)

                    # Download the blob to the local path
                    blob.download_to_filename(local_file_path)
                    print(f"Downloaded {blob.name} to {local_file_path}")
                    csv_file_path = local_file_path.replace('.h5', '.csv')
                    df = convert_h5_to_npy(local_file_path, 100)
                    all_dataframes.append(df)

                    # Delete the .h5 file after processing
                    os.remove(local_file_path)
                    print(f"Deleted H5 file: {local_file_path}")

    else:
      for letter2 in string.ascii_uppercase:  # A to Z
            # Construct the prefix dynamically
            file_name = f'MillionSongSubset/B/B/{letter1}/{letter2}/'
            destination_path = f'/content/final/B/B/{letter1}/{letter2}/'
            # Set the destination path dynamically based on the current letters

            # Ensure the local directory exists before downloading files
            if not os.path.exists(destination_path):
                    os.makedirs(destination_path)
                    # List blobs with the current prefix
            blobs = bucket.list_blobs(prefix=file_name)

                # Download each blob, convert to .npy data, and save only as CSV
            for blob in blobs:
                    # Calculate the relative path for local storage
                    relative_path = blob.name[len(file_name):]
                    local_file_path = os.path.join(destination_path, relative_path)

                    # Ensure the local directory exists before downloading
                    local_dir = os.path.dirname(local_file_path)
                    if not os.path.exists(local_dir):
                        os.makedirs(local_dir)

                    # Download the blob to the local path
                    blob.download_to_filename(local_file_path)
                    print(f"Downloaded {blob.name} to {local_file_path}")

                    #merged_df = convert_h5_to_npy(local_file_path)
                    #merged_df.to_csv('A/{letter1}/{letter2}.csv', index=False)
                    csv_file_path = local_file_path.replace('.h5', '.csv')
                    #save_npy_to_csv(features, csv_file_path)
                    df = convert_h5_to_npy(local_file_path, 100)

                    all_dataframes.append(df)

                    # Delete the .h5 file after processing
                    os.remove(local_file_path)
                    print(f"Deleted H5 file: {local_file_path}")

combine_csv_filesB()

msd_data = pd.concat(all_dataframes, ignore_index=True)

msd_data = pd.concat(all_dataframes, ignore_index=True)

msd_data.head(5)

"""Note - artist and album data was extracted using LastFM API. Using api like similar artist, the dataset was created iteratively. The threshold for selecting similar artist was kept less so that data obtained would be more diverse regarding artist popularity"""

artist_data = pd.read_csv('/content/artist_info_final.csv') # refer to another code file submitted how this dataframe was created (Data Acquistion Part)

artist_data.drop(columns=['Unnamed: 0', 'Unnamed: 9'], inplace = True)

artist_data.head(5)

album_data = pd.read_csv('/content/album_data.csv') # refer to another code file submitted how this dataframe was created (Data Acquistion Part)

album_data.head(5)

user_data = pd.read_excel('/content/output_final_user.xlsx') # refer to another code file submitted how this dataframe was created (Data Acquistion Part)

user_data.head(5)

"""# Data Visaulization and Exploratory Data Analysis

### 1. Playcont and Listeners distribution across artist and album dataset

#### Findings - Both distribution of listners and play counts are right skewed showing most songs have low listners count while some songs have huge number of audience who listen to these songs again and again
"""

sns.set(style="whitegrid")
numerical_columns = ['listeners', 'playcount']

# Create subplots for each feature
fig, axes = plt.subplots(1, len(numerical_columns), figsize=(12, 5))

# Plotting histograms and boxplots for each column
for i, col in enumerate(numerical_columns):
    # Histogram
    sns.histplot(album_data[col], kde=True, ax=axes[i], color="skyblue", bins=30)
    axes[i].set_title(f'Distribution of {col.capitalize()}')
    axes[i].set_xlabel(col.capitalize())
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""### 2. Top 10 artist by play count, listners, engagement ration
#### Findings - Artists like Coldplay and Rihanna have large listener bases but are absent from the engagement ratio chart, indicating casual or one-time listeners. Artists like BTS and Taylor Swift excel in both listeners and engagement, showcasing their ability to attract and maintain highly engaged audiences.
"""

fig, axes = plt.subplots(1, 3, figsize=(35, 8))
# Top 10 artists by listeners
top_listeners = artist_data.nlargest(10, 'listeners')
sns.barplot(x='listeners', y='Name', data=top_listeners, ax=axes[0])
axes[0].set_title('Top 10 Artists by Listeners')
axes[0].set_xlabel('Listeners')
axes[0].set_ylabel('Artist')

# Top 10 artists by play counts
top_play_counts = artist_data.nlargest(10, 'play_counts')
sns.barplot(x='play_counts', y='Name', data=top_play_counts, ax=axes[1])
axes[1].set_title('Top 10 Artists by Play Counts')
axes[1].set_xlabel('Play Counts')
axes[1].set_ylabel('Artist')

# Engagement Ratio
artist_data['engagement_ratio'] = artist_data['play_counts'] / artist_data['listeners']
top_engaged = artist_data.nlargest(10, 'engagement_ratio')
sns.barplot(x='engagement_ratio', y='Name', data=top_engaged)
axes[2].set_title('Top 10 Artists by Engagement Ratio')
axes[2].set_xlabel('Engagement Ratio')
axes[2].set_ylabel('Artist')
plt.show()


plt.tight_layout()
plt.show()

"""### 3. Common Tags given to album and artists by users
#### Findings - Albums are generally classified with their genre like Hip-Hop and rock. While it comes to artists, it depends on artist characteritcs like if users have seen the artist perform live or if artist is female or male vocalist.
"""

# Generate word cloud for album_data
album_tag_text = ' '.join(album_data['tags'].str.replace(r"[\[\]',]", '').str.lower())
album_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(album_tag_text)

# Generate word cloud for artist_data
artist_tag_text = ' '.join(artist_data['tags'].str.replace(r"[\[\]',]", '').str.lower())
artist_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(artist_tag_text)

# Plot the word clouds side-by-side
fig, axes = plt.subplots(1, 2, figsize=(20, 10))

axes[0].imshow(album_wordcloud, interpolation='bilinear')
axes[0].axis('off')
axes[0].set_title('Album Data WordCloud')

axes[1].imshow(artist_wordcloud, interpolation='bilinear')
axes[1].axis('off')
axes[1].set_title('Artist Data WordCloud')

plt.show()

"""### 4. Trend between listeners count and play count
#### Key Findings -  As listeners count ( no of unique listeners) increased the play count ( no of time a song has been played) also increases. Outliers with high play counts but low listeners are worth investigating as they may indicate a niche but highly engaged audience.

"""

# Calculate z-scores for listeners and play counts
artist_data['listeners_zscore'] = zscore(artist_data['listeners'])
artist_data['play_counts_zscore'] = zscore(artist_data['play_counts'])

# outliers as data points with z-scores > 5.5
outliers = artist_data[(np.abs(artist_data['listeners_zscore']) > 5.5) | (np.abs(artist_data['play_counts_zscore']) > 5.5)]

# Creating a horizontal layout for two scatter plots
fig, axes = plt.subplots(1, 2, figsize=(20, 6))

# Scatter plot for Play Counts vs Listeners (without highlighting outliers)
sns.scatterplot(x='listeners', y='play_counts', data=artist_data, alpha=0.5, ax=axes[0])
axes[0].set_title('Play Counts vs Listeners')
axes[0].set_xlabel('Listeners')
axes[0].set_ylabel('Play Counts')

# Scatter plot for Play Counts vs Listeners with outliers highlighted
sns.scatterplot(x='listeners', y='play_counts', data=artist_data, alpha=0.5, label='Data Points', ax=axes[1])
axes[1].scatter(outliers['listeners'], outliers['play_counts'], edgecolor='red', facecolor='none', s=100, label='Outliers')
axes[1].set_title('Play Counts vs Listeners with Outliers Highlighted')
axes[1].set_xlabel('Listeners')
axes[1].set_ylabel('Play Counts')
axes[1].legend()

plt.tight_layout()
plt.show()

"""### 5. User data distribution by Country
#### Findings - Most data collected are from United Kingdom followed by United States, so data obtained will be have artist prominently popular in these countries
"""

# Drop rows where 'Country' is None or NaN
country_counts = user_data['Country'].dropna().value_counts()

plt.figure(figsize=(14, 8))
sns.barplot(x=country_counts.index, y=country_counts.values, palette='viridis')

plt.title('User Distribution by Country')
plt.xlabel('Country')
plt.ylabel('Number of Users')
plt.xticks(rotation=90, ha='right')  # Rotate the labels and align them to the right
plt.tight_layout()
plt.show()

"""# Data Cleaning"""

# Create subplots with shared y-axis (1 row, 3 columns)
fig, axes = plt.subplots(1, 3, figsize=(25, 5), sharey=True)


sns.kdeplot(msd_data['duration'], ax=axes[0], color='green')
axes[0].set_title('Duration')

sns.kdeplot(msd_data['start_of_fade_out'], ax=axes[1], color='red')
axes[1].set_title('Start of Fade Out')

sns.kdeplot(msd_data['loudness'], ax=axes[2], color='purple')
axes[2].set_title('Loudness')

# Set overall labels and layout
plt.tight_layout()
plt.show()

"""### Converting variables to right encoding for readability and dropping columns with lot of NAN values"""

# Dropping multiple columns with NaN values
columns_to_drop = ['idx_similar_artists', 'idx_artist_terms', 'genre',
                   'analysis_sample_rate', 'energy', 'analyzer_version', 'danceability']
msd_data.drop(columns=columns_to_drop, inplace=True)
# Decoding specific columns
columns_to_decode = ['title', 'song_id', 'release', 'artist_name', 'artist_mbid', 'artist_id', 'track_id']
msd_data[columns_to_decode] = msd_data[columns_to_decode].applymap(lambda x: x.decode('utf-8'))

# Filter rows where specified columns have non-empty lists
columns_to_check = ['beat_start', 'beat_confidence', 'bar_confidence',
                    'sections_confidence', 'tatums_start']

for col in columns_to_check:
    msd_data = msd_data[msd_data[col].apply(lambda x: len(x)) > 0]


msd_data['artist_mbtags'] = msd_data['artist_mbtags'].apply(lambda x: ["Unknown"] if len(x) == 0 else [item.decode('utf-8') for item in x])
msd_data['artist_mbtags_counts'] = msd_data['artist_mbtags_counts'].apply(lambda x: [0] if len(x) == 0 else x)

"""# Machine Learning and Data Mining

# Model 1  Melodify clusterting based on embeddings

### The audio features of the Million Song Data are converted into embeddings using a Convolutional Neural Network (CNN). These embeddings are then clustered to form distinct groups.
"""

feature_columns = ["bar_confidence", "beat_confidence", "sections_confidence"]
df = msd_data[feature_columns]

def parse_and_pad(column, pad_length):
    sequences = df[column].tolist()
    return pad_sequences(sequences, maxlen=pad_length, padding='post', dtype='float32')

pad_length = 20
# Process each feature column
features = [parse_and_pad(col, pad_length) for col in feature_columns]
# Combine features into a single tensor
X = np.stack(features, axis=0)
print(X.shape)
X = X[..., np.newaxis]

print("Input shape:", X.shape)
X = np.moveaxis(X, 0, 1)

n_features, sequence_length = X.shape[1:3]
embedding_size = 50

model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(n_features, sequence_length, 1), padding='same'),
    MaxPooling2D((2, 2), padding='same'),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    MaxPooling2D((2, 2), padding='same'),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(embedding_size)
])
model.compile(optimizer='adam', loss='mse')
y = np.random.rand(X.shape[0], embedding_size).astype(np.float32)
model.fit(X, y, epochs=20, batch_size=32)

# Generate embeddings
embeddings = model.predict(np.transpose(X, (0, 2, 1, 3)))
print("Embeddings shape:", embeddings.shape)

model.save('song_embedding_model.keras')
embedding_df = pd.DataFrame(embeddings)
embedding_df.to_csv("song_embeddings.csv", index=False)

embeddings = pd.read_csv("song_embeddings.csv").values
# Determining the optimal number of clusters using the Elbow method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)
    kmeans.fit(embeddings)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.show()

k = 3
kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)
clusters = kmeans.fit_predict(embeddings)
plt.figure(figsize=(8, 6))
plt.scatter(embeddings[:, 0], embeddings[:, 1], c=clusters, cmap='viridis', s=50)
plt.title('Song Embeddings Clusters')
plt.xlabel('Embedding Dimension 1')
plt.ylabel('Embedding Dimension 2')
plt.colorbar(label='Cluster')
plt.show()

# Using PCA to convert n dimension embedding into 2-D vector
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings)
plt.figure(figsize=(8, 6))
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='viridis', s=50)
plt.title('Song Embeddings Clusters (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

silhouette_avg = silhouette_score(embeddings, clusters)

print(f"The average silhouette score is: {silhouette_avg}")
song_clusters = pd.DataFrame({'title': msd_data['title'], 'cluster': clusters})

# Print 5 random song names from each cluster
for cluster_id in range(k):
    cluster_songs = song_clusters[song_clusters['cluster'] == cluster_id]
    print(f"Cluster {cluster_id}:")

    # Handle cases where a cluster might have fewer than 5 songs
    num_songs_to_print = min(10, len(cluster_songs))

    random_songs = cluster_songs.sample(n=num_songs_to_print, random_state=0) # Set random_state for reproducibility
    for index, row in random_songs.iterrows():
        print(row['title'])
    print("\n")

# Find the index of "L'idole des femmes"

target_song_index = msd_data[msd_data['title'] == "L.A. Arteest Caf√©"].index

if not target_song_index.empty:
    target_song_index = target_song_index[0]
    # Scale the embeddings
    scaler = StandardScaler()
    scaled_embeddings = scaler.fit_transform(embeddings)
    # Calculate cosine similarity
    similarity_scores = cosine_similarity([scaled_embeddings[target_song_index]], scaled_embeddings)
    # Find the index of the most similar song (excluding the target song itself)
    most_similar_index = np.argsort(similarity_scores[0])[-2]
    # Get the title of the most similar song
    most_similar_song_title = msd_data.loc[most_similar_index, 'title']
    print(f"The closest song to 'Lady Marmalade' is: {most_similar_song_title}")

else:
    print("'L'idole des femmes' not found in the dataset.")

"""# Model 2 - Using XGBoost model for recomendations
### User preferences based on tags are used to recommend songs. The tags are converted into embeddings using a Sentence Transformer. These embeddings are then utilized to cluster the data into 10 groups. Finally, XGBoost is employed to predict songs corresponding to the given tags.
"""

def preprocess_text(text):
  if pd.isna(text):
    return []
  tokens = word_tokenize(text.lower())
  tokens = [word for word in tokens if word not in string.punctuation and word not in stop_words]
  return tokens

artist = artist_data[:1000]
artist['summary_tokens'] = artist['summary'].apply(preprocess_text)
artist['top_album_tokens'] = artist['summary'].apply(preprocess_text)
artist['top_songs_tokens'] = artist['summary'].apply(preprocess_text)
corpus = (
    artist['summary_tokens'].tolist() +
    artist['top_album_tokens'].tolist() +
    artist['top_songs_tokens'].tolist()
)

# usng Word2Vec for conversion to vector
word2Vec_model = Word2Vec(
    sentences = corpus,
    vector_size = 100,
    window = 5,
    workers = 1,
    sg = 1
)
def get_feature_vector(token, models):
  vectors = [models.wv[word] for word in token if word in models.wv]
  if len(vectors) == 0:
      return np.zeros(models.vector_size)
  return np.mean(vectors, axis = 0)

artist['summary_vector'] = artist['summary_tokens'].apply(lambda x: get_feature_vector(x, word2Vec_model))
artist['top_album_vector'] = artist['top_album_tokens'].apply(lambda x: get_feature_vector(x, word2Vec_model))
artist['top_song_vector'] = artist['top_songs_tokens'].apply(lambda x: get_feature_vector(x, word2Vec_model))

similar_artist_list = artist['similar_artist'].tolist()
model = Word2Vec(sentences = similar_artist_list, vector_size = 100,
                 window = 5, min_count=1, sg = 1, workers=4)
artist['similar_artist_embedding'] = artist['similar_artist'].apply(lambda x: get_feature_vector(x, model))

# inverse transformer so model give more weightage to artist will lesser play counts
artist['listeners'] = 1 / (artist['listeners'] + 1e-6)
artist['play_counts'] = 1 / (artist['play_counts'] + 1e-6)

# top 5 tags of each artist is taken into consideration and unique tag list is made
artist['tags'] = artist['tags'].apply(lambda x: x[:5] if isinstance(x, list) else x)
artist['tags'] = artist['tags'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
artist['tags'] = artist['tags'].apply(lambda x: list(set(tag.lower() for tag in x)))
unique_tags = set(tag for tag_list in artist['tags'] for tag in tag_list)
unique_tags = list(unique_tags)

# loading the Sentence Transformer
model = SentenceTransformer('all-MiniLM-L6-v2')

tag_embeddings = model.encode(unique_tags)
cos_sim = cosine_similarity(tag_embeddings)
linkage_matrix = linkage(tag_embeddings, method='ward')
plt.figure(figsize=(10, 7))
dendrogram(linkage_matrix, labels=unique_tags, leaf_rotation=90)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Tags")
plt.ylabel("Distance")
plt.show()

# using clustering to obtain 10 clusters that will divide songs into groups
clusterer = AgglomerativeClustering(n_clusters=10, metric='euclidean', linkage='ward')
clusters = clusterer.fit_predict(tag_embeddings)
tag_clusters = {tag: cluster for tag, cluster in zip(unique_tags, clusters)}
result = {}
for key, value in tag_clusters.items():
    if value not in result:
      result[value] = []
    result[value].append(key)

# List of genre names
result1 = {}
genres = ['hip hop', 'romantic', 'lounge music', 'instrumental', 'singer',
          'rock', 'pop', 'country music', 'hardcore', 'metal']

# Create result1 by mapping genre names to the corresponding values from result and according to count of tags in each cluster
result1 = {genre: result[idx] for idx, genre in enumerate(genres)}
for genre, values in result1.items():
    print(f"'{genre}' has {len(values)} values")

tag_to_cluster = {tag : cluster for cluster, tags in result1.items() for tag in tags}
def map_tag_to_cluster(tag_list):
  preference_order = ['metal', 'pop', 'instrumental', 'rock', 'country music','instrumental', 'country music',  'lounge music', 'singer', 'hip hop', 'romantic']
  mapped_clusters = [tag_to_cluster[tag] for tag in tag_list if tag in tag_to_cluster]
  mapped_clusters = list(set(mapped_clusters))
  for cluster in mapped_clusters:
    if cluster in preference_order:
      return str(cluster)
artist['mapped_cluster'] = artist['tags'].apply(map_tag_to_cluster)
sns.barplot(artist['mapped_cluster'])

# converting to correct labels
label_encoder = LabelEncoder()
artist['mapped_cluster_encoded'] = label_encoder.fit_transform(artist['mapped_cluster'])

artist.drop(columns=['MBID', 'similar_artist', 'tags', 'summary', 'top_album', 'summary_tokens', 'top_album_tokens', 'top_songs_tokens', 'mapped_cluster', 'top_songs'], inplace=True)
features = np.hstack([
    np.vstack(artist['summary_vector']),
    np.vstack(artist['top_album_vector']),
    np.vstack(artist['top_song_vector']),
    np.vstack(artist['similar_artist_embedding']),
    artist[['listeners', 'play_counts']].values
    ])

target = artist['mapped_cluster_encoded'].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

from xgboost import XGBClassifier
model = XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    objective='multi:softmax',  # Use 'multi:softprob' for probabilities
    num_class=10,  # Number of unique tags
    random_state=42
)
model.fit(X_train, y_train)

# Predictions
from sklearn.metrics import accuracy_score, f1_score, classification_report
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print(classification_report(y_test, y_pred))

# given a tag it will give top 10 artist belongs to those genre, the result obtained shows artist with ranging playcount showing the robustness of model
tag = 1 # romantics
ranked_artists_names = []
tag_probs = model.predict_proba(X_test)
ranked_artists = np.argsort(-tag_probs[:, tag])  # Rank artists by probabilities
top_name = [artist['Name'].iloc[i] for i in ranked_artists[:10]]
ranked_artists_names.extend(top_name)
print("Top Recommended Artists for Tag", tag, ":", ranked_artists_names)

def precision_at_k(y_true, y_pred, k):
    correct = 0
    for i in range(len(y_true)):
        if y_true[i] in y_pred[i][:k]:
            correct += 1
    return correct / len(y_true)

precision = precision_at_k(y_test, [np.argsort(-probs) for probs in tag_probs], k=5)
print("Precision@5:", precision)

"""# Model 3 - Using RNN to understand similarity between songs
### The model leverages a Recurrent Neural Network (RNN) to capture patterns in the audio features and predict the similarity between songs.
"""

# Features
features = ['segments_confidence', 'segments_loudness_max', 'bar_confidence',
            'beat_confidence', 'tempo', 'loudness', 'key', 'mode']

df_selected = msd_data[features].copy()

# Handle missing values
numeric_columns = df_selected.select_dtypes(include=['float64', 'int32'])
df_selected[numeric_columns.columns] = numeric_columns.fillna(numeric_columns.mean())

categorical_columns = df_selected.select_dtypes(include=['object'])
label_encoder = LabelEncoder()

# Encoding categorical columns like 'key' and 'mode'
for column in categorical_columns.columns:
    df_selected[column] = label_encoder.fit_transform(df_selected[column].astype(str))

# Normalizing the data
scaler = StandardScaler()
X_normalized = scaler.fit_transform(df_selected)  # Normalizing the feature data

# Applying PCA for dimensionality reduction (reduce to 2 components for visualization)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_normalized)

# Spliting the dataset into training (80%) and testing (20%)
X_train, X_test = train_test_split(X_reduced, test_size=0.2, random_state=42)

# Creating an RNN model for the task
model = models.Sequential()
model.add(layers.InputLayer(input_shape=(X_train.shape[1], 1)))
model.add(layers.LSTM(128, activation='relu', return_sequences=True))
model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
model.add(layers.LSTM(64, activation='relu'))
model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(X_train.shape[1]))

model.compile(optimizer='adam', loss='mse')

# Reshaping the data for RNN input (samples, timesteps, features)
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Set up EarlyStopping callback to stop training as soon as the loss reaches its lowest level
early_stopping = EarlyStopping(monitor='val_loss', patience=0, restore_best_weights=True)
history = model.fit(X_train_reshaped, X_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

# Step 8: Evaluate the model on the test set
test_loss = model.evaluate(X_test_reshaped, X_test)
print(f"Test Loss: {test_loss}")

# Plot training history
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# Function to calculate similarity between songs
def get_similarity_matrix(X_reduced):
    return cosine_similarity(X_reduced)

# Get similarity matrix for the entire dataset
similarity_matrix = get_similarity_matrix(X_reduced)

# Get true similar songs from the similarity matrix (ground truth)
# For simplicity, let's define the ground truth as the top 5 similar songs based on cosine similarity
ground_truth_similar_songs = []
for i in range(len(X_test)):
    song_similarities = similarity_matrix[i]
    similar_songs = song_similarities.argsort()[-6:-1]  # Excluding the song itself (top 5 similar songs)
    ground_truth_similar_songs.append(similar_songs)

# Recommend similar songs based on the model's similarity matrix (predictions)
def recommend_similar_songs(song_idx, similarity_matrix, n=5):
    similar_indices = similarity_matrix[song_idx].argsort()[-(n+1):-1]
    return similar_indices

#  Get recommendations for all songs in the test set
recommendations = []
for song_idx in range(len(X_test)):  # Loop through all test set songs
    similar_songs = recommend_similar_songs(song_idx, similarity_matrix, n=5)
    recommendations.append(similar_songs)

#  Display the recommendations (song titles instead of indices)
# Assuming `data` has a column 'title' that contains the song titles
song_titles = msd_data['title'].tolist()

# Create a DataFrame for song titles and their top similar songs
similarity_data = []
for idx, recs in enumerate(recommendations):
    song_title = song_titles[idx]
    for rec_idx in recs:
        similar_song_title = song_titles[rec_idx]
        similarity_data.append([song_title, similar_song_title])

# Create a DataFrame
similarity_df = pd.DataFrame(similarity_data, columns=['Song', 'Most_Similar_Song'])

# Display the DataFrame
print(similarity_df)

 # Ensure the number of test samples is consistent
# Ensure that the number of test samples is consistent
num_test_samples = len(ground_truth_similar_songs)  # Ground truth length (test set size)

# Trim recommendations to match the number of test samples
recommendations_trimmed = recommendations[:num_test_samples]

similarity_df_matrix = pd.DataFrame(similarity_matrix, columns=msd_data['title'], index=msd_data['title'])

# Print the similarity matrix
print("Similarity Matrix (Top 10 Songs Example):")
similarity_df_matrix.head(10)

"""# Model 4 Using random forest to decide the feature importance
### A Random Forest model is used to identify the importance of various features. To optimize the split values of the features, K-Nearest Neighbors (KNN) is applied to minimize the error.
"""

features = ['segments_confidence', 'segments_loudness_max', 'bar_confidence',
            'beat_confidence', 'tempo', 'loudness', 'key', 'mode']  # Use all features


df_selected = msd_data[features].copy()

numeric_columns = df_selected.select_dtypes(include=['float64', 'int32'])
df_selected[numeric_columns.columns] = numeric_columns.fillna(numeric_columns.mean())

# Handle categorical columns by filling missing values with the mode
categorical_columns = df_selected.select_dtypes(include=['object'])
label_encoder = LabelEncoder()

# Encoding categorical columns like 'key' and 'mode'
for column in categorical_columns.columns:
    df_selected[column] = label_encoder.fit_transform(df_selected[column].astype(str))


scaler = StandardScaler()
X_normalized = scaler.fit_transform(df_selected)

# Apply PCA for dimensionality reduction (reduce to 2 components for visualization)
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X_normalized)

# Split the dataset into training (80%) and testing (20%)
X_train, X_test = train_test_split(X_reduced, test_size=0.2, random_state=42)

# Train the Random Forest model to identify important features
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, X_train)

# Use KNN to find similar songs based on all features
knn_model = KNeighborsRegressor(n_neighbors=5)
knn_model.fit(X_train, X_train)

# Find similar songs using KNN based on all features for each test song
similar_songs = knn_model.kneighbors(X_test, n_neighbors=5)

# Create a DataFrame to store the test song and its similar songs
similarity_data = []

# Iterate over each test song to collect its top 5 similar songs
for i, song_index in enumerate(range(len(X_test))):
    similar_song_indices = similar_songs[1][i]
    similar_song_titles = msd_data['title'].iloc[similar_song_indices].tolist()

    # Prepare the data for this test song
    similarity_data.append({
        'Test Song': msd_data['title'].iloc[song_index],
        'Predicted Top 1 Similar Song': similar_song_titles[0],
        'Predicted Top 2 Similar Song': similar_song_titles[1],
        'Predicted Top 3 Similar Song': similar_song_titles[2],
        'Predicted Top 4 Similar Song': similar_song_titles[3],
        'Predicted Top 5 Similar Song': similar_song_titles[4]
    })

#Convert the similarity data into a DataFrame
similarity_df = pd.DataFrame(similarity_data)

# Visualizing the similarity of songs: Create a scatter plot of test songs and their closest similar songs
plt.figure(figsize=(10, 6))
for idx, similar_song_indices in enumerate(similar_songs[1]):
    # Plot the test song
    plt.scatter(X_test[idx, 0], X_test[idx, 1], color='red', marker='o', label=f'Test Song {idx}')

    # Plot the similar songs (ensure indices are within bounds of X_test)
    for similar_song_idx in similar_song_indices:
        if similar_song_idx < len(X_test):
            plt.scatter(X_test[similar_song_idx, 0], X_test[similar_song_idx, 1], color='blue', marker='x')

plt.title('Test Songs and Their Predicted Similar Songs')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
#plt.legend(loc='upper right')
plt.show()

similarity_df

"""# Model 5 Tag to Word2Vec Embedding
### This model averages the Word2Vec embeddings of keywords to generate a song vector, representing the song based on its associated tags.
"""

# get the unique tags from all artists and albums
unique_tags = set()
for tags in artist_data['tags']:
    val = tags.replace('[', '').replace(']', '').replace("'", '').split(', ')
    unique_tags.update(val)

for tags in album_data['tags']:
    val = tags.replace('[', '').replace(']', '').replace("'", '').split(', ')
    unique_tags.update(val)

print(len(unique_tags))

# Load the pre-trained Word2Vec model
try:
    model = api.load('word2vec-google-news-300')
except Exception as e:
    print(f"Error loading word2vec model: {e}")
    print("Trying to download the model...")
    model = api.load('word2vec-google-news-300')


def get_embedding(tags):
    embeddings = []
    for tag in word_tokenize(tags):
      try:
        embeddings.append(model[tag])
      except KeyError:
          # Handle words not in vocabulary (e.g., use a zero vector or a special token)
          embeddings.append(np.zeros(300))
    return np.mean(embeddings, axis=0) if embeddings else np.zeros(300)


album_data['tag_embedding'] = album_data['tags'].apply(lambda x: get_embedding(x.replace('[', '').replace(']', '').replace("'", '')))
artist_data['tag_embedding'] = artist_data['tags'].apply(lambda x: get_embedding(x.replace('[', '').replace(']', '').replace("'", '')))

# Combine tag embeddings from albums and artists
tag_embeddings = album_data['tag_embedding'].tolist()

# Determine optimal k using the elbow method
inertia = []
silhouette_avg = []
K = range(2, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(tag_embeddings)
    inertia.append(kmeans.inertia_)

    # Calculate silhouette score
    cluster_labels = kmeans.labels_
    if len(np.unique(cluster_labels)) > 1:
      silhouette_avg.append(silhouette_score(tag_embeddings, cluster_labels))
    else:
      silhouette_avg.append(0)

# Plot the elbow method graph
plt.figure(figsize=(10, 5))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('The Elbow Method showing the optimal k')
plt.show()

#Plot the silhouette method graph
plt.figure(figsize=(10, 5))
plt.plot(K, silhouette_avg, 'bx-')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method showing the optimal k')
plt.show()

num_clusters = 7

kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=10)
kmeans.fit(album_data['tag_embedding'].tolist())
# Add cluster labels to the DataFrames
album_data['cluster'] = kmeans.labels_[:len(album_data)]

# Now you have the cluster labels in 'cluster' column of both DataFrames
album_data.head()

# Apply PCA to reduce dimensions to 2
pca = PCA(n_components=2)
pca_result = pca.fit_transform(album_data['tag_embedding'].tolist())
plt.figure(figsize=(10, 8))
for i in range(num_clusters):
    plt.scatter(pca_result[album_data['cluster'] == i, 0], pca_result[album_data['cluster'] == i, 1], label=f'Cluster {i}')

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('KMeans Clustering of Album Tags')
plt.legend()
plt.show()

def print_random_songs_from_clusters(albums_df, num_clusters, num_songs=10):
    #Prints a specified number of random songs from each cluster


    for i in range(num_clusters):
        cluster_songs = albums_df[albums_df['cluster'] == i]

        random_indices = random.sample(range(len(cluster_songs)), min(num_songs, len(cluster_songs)))
        print(f"Cluster {i}:")
        for index in random_indices:
          print(cluster_songs.iloc[index]['tracks'])
        print("-" * 20)

print_random_songs_from_clusters(album_data, num_clusters)

def find_closest_song(albums_df, target_song):
    # Find the target song in the DataFrame
    target_song_index = albums_df[albums_df['tracks'].str.contains(target_song, na=False)].index

    if not target_song_index.empty:
        target_song_embedding = albums_df.loc[target_song_index[0], 'tag_embedding']

        # Calculate cosine similarity between the target song and all other songs
        similarities = []
        for index, row in albums_df.iterrows():
          try:
              similarity = cosine_similarity([target_song_embedding], [row['tag_embedding']])[0][0]
          except ValueError as e:
            similarity = 0.0
          similarities.append(similarity)

        # Exclude the target song itself from the similarity calculation
        similarities[target_song_index[0]] = -1

        # Find the index of the song with the highest similarity
        closest_song_index = np.argmax(similarities)

        # Return the name of the closest song
        return albums_df.iloc[closest_song_index]['tracks']
    else:
        return None


target_song = "Kiss Me On My Neck"
closest_song = find_closest_song(album_data, target_song)

if closest_song:
    print(f"The closest song to '{target_song}' is: {closest_song}")
else:
    print(f"Song '{target_song}' not found in the dataset.")

def print_top_songs_per_cluster(albums_df, num_songs=5):
    # Calculate the 99th percentile of playcount
    percentile_99 = albums_df['playcount'].quantile(0.99)
    print(f"99th Percentile Playcount: {percentile_99}")

    # Filter for songs in the top 1%
    top_1_percent = albums_df[albums_df['playcount'] >= percentile_99]


    for cluster_id in top_1_percent['cluster'].unique():
        cluster_songs = top_1_percent[top_1_percent['cluster'] == cluster_id]

        # Sort by playcount in descending order and take the top 'num_songs'
        top_songs = cluster_songs.sort_values(by='playcount', ascending=False).head(num_songs)

        print(f"\nTop {num_songs} songs in Cluster {cluster_id}:")
        for index, row in top_songs.iterrows():
            print(f"- {row['tracks']} (Playcount: {row['playcount']})")


print_top_songs_per_cluster(album_data)

"""# Model 6 Graph Embedding on tags, tracks and artist
### In this model, a graph is constructed using features such as tags, tracks, and artists. Node2Vec is then applied to the track nodes to generate embeddings that capture the relationships and similarities between tracks
"""

def create_network(albums_df, artists_df):

    graph = nx.Graph()
    albums_df['tracks'] = albums_df['tracks'].astype(str)
    # Add nodes for tags
    for tags in albums_df['tags']:
        for tag in tags.replace('[', '').replace(']', '').replace("'", '').split(', '):
            tag = tag.strip()
            if tag:
                graph.add_node(tag, type='tag')

    for tracks in albums_df['tracks']:
        graph.add_node(tracks, type='track')

    for tags in artists_df['tags']:
        for tag in tags.replace('[', '').replace(']', '').replace("'", '').split(', '):
            tag = tag.strip()
            if tag:
                graph.add_node(tag, type='tag')


    # Add nodes for artists and albums and connect them to tags
    for index, row in albums_df.iterrows():
        album_name = row['album'].lower()
        track_name = row['tracks'].lower()
        artist_name = row['artist'].lower()
        tags = row['tags'].replace('[', '').replace(']', '').replace("'", '').split(', ')
        graph.add_node(album_name, type='album')
        graph.add_edge(album_name, artist_name)
        graph.add_edge(track_name, artist_name)
        graph.add_edge(track_name, album_name)

        for tag in tags:
            tag = tag.strip().lower()
            if tag:
                graph.add_edge(track_name, tag)

    for index, row in artists_df.iterrows():
        artist_name = row['Name'].lower()
        tags = row['tags'].replace('[', '').replace(']', '').replace("'", '').split(', ')

        #Ensure artist node is added
        if not graph.has_node(artist_name):
            graph.add_node(artist_name, type='artist')

        for tag in tags:
            tag = tag.strip()
            if tag:
                graph.add_edge(artist_name, tag)

    return graph

music_network = create_network(album_data, artist_data)

subset_nodes = ['the weeknd']
for node in music_network.neighbors('the weeknd'):
    subset_nodes.append(node)

# Create a subgraph with the selected nodes and their connections
subset_graph = music_network.subgraph(subset_nodes)

# Assign colors based on node type
node_colors = []
for node in subset_graph.nodes:
    node_type = subset_graph.nodes[node].get('type', 'unknown')
    if node_type == 'artist':
        node_colors.append('red')
    elif node_type == 'track':
        node_colors.append('blue')
    elif node_type == 'tag':
        node_colors.append('green')
    elif node_type == 'album':
        node_colors.append('purple')
    else:
        node_colors.append('gray')

# Draw the subgraph with color-coded nodes
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(subset_graph)
nx.draw(
    subset_graph,
    pos,
    with_labels=True,
    node_size=500,
    node_color=node_colors,
    font_size=8
)
plt.title("Subgraph of Music Network (Color-Coded by Node Type)")
plt.savefig("music_network_subgraph.png")
plt.show()

!pip install node2vec

from node2vec import Node2Vec
node2vec = Node2Vec(music_network, dimensions=10, walk_length=5, num_walks=5, workers=1)
model = node2vec.fit(window=2, min_count=1, batch_words=1)

# Function to get embedding for a song
def get_song_embedding(song_name, model):
    try:
        return model.wv[song_name.lower()]  # Use .lower() for consistency
    except KeyError:
        print(f"Warning: Embedding not found for song '{song_name}'. Returning a zero vector.")
        return np.zeros(model.wv.vector_size)

# Apply the function to each song in the DataFrame
album_data['node2vec_embedding'] = album_data['tracks'].apply(lambda x: get_song_embedding(x, model))

# Now the 'albums_df' DataFrame contains a new column 'node2vec_embedding' with the embeddings.
album_data.head()

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def find_closest_songs(albums_df, target_song, top_n=5):
    """
    Finds the top_n closest songs to the target song based on cosine similarity of node2vec embeddings.

    Args:
        albums_df: DataFrame containing album information, including 'tracks' and 'node2vec_embedding'.
        target_song: The name of the target song.
        top_n: The number of closest songs to return.

    Returns:
        A list of tuples, where each tuple contains the name of a song and its cosine similarity to the target song.
        Returns an empty list if the target song is not found or if there's an error.
    """

    try:
        # Find the target song in the DataFrame
        target_song_index = albums_df[albums_df['tracks'].str.contains(target_song, na=False)].index
        if target_song_index.empty:
            # print(f"Song '{target_song}' not found in the dataset.")
            return []

        target_song_embedding = albums_df.loc[target_song_index[0], 'node2vec_embedding']

        # Calculate cosine similarity between the target song and all other songs
        similarities = []
        for index, row in albums_df.iterrows():
            try:
                similarity = cosine_similarity([target_song_embedding], [row['node2vec_embedding']])[0][0]
                similarities.append((row['tracks'], similarity))
            except ValueError as e:
                print(f"Error calculating similarity for song {row['tracks']}: {e}")
                similarities.append((row['tracks'], 0.0))  # Assign 0 similarity in case of error


        # Sort songs by similarity in descending order
        similarities.sort(key=lambda x: x[1], reverse=True)

        # Exclude the target song itself
        closest_songs = [song for song in similarities if song[0] != target_song][:top_n]

        return closest_songs
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

# Example usage
target_song = "Kiss Me On My Neck" # Example song
closest_songs = find_closest_songs(album_data, target_song)

if closest_songs:
    print(f"The {len(closest_songs)} closest songs to '{target_song}' are:")
    for song, similarity in closest_songs:
        print(f"- {song} (Similarity: {similarity:.4f})")

track_vectors = album_data['node2vec_embedding'].tolist()
# Perform KMeans clustering with k=7
kmeans = KMeans(n_clusters=7, random_state=0, n_init=10)
kmeans.fit(track_vectors)

# Add cluster labels to the DataFrame
album_data['track_cluster'] = kmeans.labels_

# Print the updated DataFrame with cluster assignments
album_data.head()

# Apply PCA to reduce dimensions to 2
pca = PCA(n_components=2)
pca_result = pca.fit_transform(album_data['node2vec_embedding'].tolist())

# Create a scatter plot of the clusters
plt.figure(figsize=(10, 8))
for i in range(7):
    plt.scatter(pca_result[album_data['track_cluster'] == i, 0], pca_result[album_data['track_cluster'] == i, 1], label=f'Cluster {i}')

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('KMeans Clustering of Song Embeddings (node2vec)')
plt.legend()
plt.savefig("kmeans_clustering_node2vec.png")
plt.show()

def evaluate_closest_song(albums_df):
    correct_predictions = 0
    total_predictions = 0

    for index, row in albums_df[:100].iterrows():
        target_song = row['tracks'].replace('(', '').replace(')', '')
        similar_tracks = row['similar_tracks']

        if pd.notna(similar_tracks):
          closest_songs = find_closest_songs(albums_df, target_song, top_n=1)

          if closest_songs:
              closest_song = closest_songs[0][0]

              if closest_song in similar_tracks:
                  correct_predictions += 1

              total_predictions += 1

    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    return accuracy

accuracy = evaluate_closest_song(album_data)
print(f"Accuracy of closest song prediction: {accuracy:.4f}")

target_song = "Kiss Me On My Neck"
closest_song = find_closest_songs(album_data, target_song)

if closest_song:
    print(f"The closest song to '{target_song}' is: {closest_song}")
else:
    print(f"Song '{target_song}' not found in the dataset.")

# prompt: give me 3 random songs from each cluster

def print_random_songs_from_clusters(albums_df, num_clusters, num_songs=3):
    """Prints a specified number of random songs from each cluster.

    Args:
        albums_df: DataFrame containing album information and cluster assignments.
        num_clusters: The total number of clusters.
        num_songs: The number of random songs to print from each cluster.
    """

    for i in range(num_clusters):
        cluster_songs = albums_df[albums_df['track_cluster'] == i]
        if not cluster_songs.empty:  # Check if the cluster has any songs
            random_indices = random.sample(range(len(cluster_songs)), min(num_songs, len(cluster_songs)))
            print(f"Cluster {i}:")
            for index in random_indices:
                print(cluster_songs.iloc[index]['tracks'])  # Assuming 'name' column contains song names
        else:
            print(f"Cluster {i} is empty.")
        print("-" * 20)

print_random_songs_from_clusters(album_data, 7, 3)